{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdLEG4r_rgY5"
      },
      "outputs": [],
      "source": [
        "!pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "model = LLM(\"selfrag/selfrag_llama2_7b\", download_dir=\"/gscratch/h2lab/akari/model_cache\", dtype=\"half\")\n",
        "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=100, skip_special_tokens=False)\n",
        "\n",
        "def format_prompt(input, paragraph=None):\n",
        "  prompt = \"### Instruction:\\n{0}\\n\\n### Response:\\n\".format(input)\n",
        "  if paragraph is not None:\n",
        "    prompt += \"[Retrieval]{0}\".format(paragraph)\n",
        "  return prompt\n",
        "\n",
        "query_1 = \"Leave odd one out: twitter, instagram, whatsapp.\"\n",
        "query_2 = \"Can you tell me the difference between llamas and alpacas?\"\n",
        "queries = [query_1, query_2]\n",
        "\n",
        "# for a query that doesn't require retrieval\n",
        "preds = model.generate([format_prompt(query) for query in queries], sampling_params)\n",
        "for pred in preds:\n",
        "  print(\"Model prediction: {0}\".format(pred.outputs[0].text))"
      ],
      "metadata": {
        "id": "dSHhZ13rr184"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"Llamas range from 200 to 350 lbs., while alpacas weigh in at 100 to 175 lbs.\"\"\""
      ],
      "metadata": {
        "id": "GLpcJ3CQsL3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt_p(input, paragraph=paragraph):\n",
        "  prompt = \"### Instruction:\\n{0}\\n\\n### Response:\\n\".format(input)\n",
        "  if paragraph is not None:\n",
        "    prompt += \"[Retrieval]{0}\".format(paragraph)\n",
        "  return prompt\n",
        "\n",
        "query_1 = \"Leave odd one out: twitter, instagram, whatsapp.\"\n",
        "query_2 = \"Can you tell me the differences between llamas and alpacas?\"\n",
        "queries = [query_1, query_2]\n",
        "\n",
        "# for a query that doesn't require retrieval\n",
        "preds = model.generate([format_prompt_p(query) for query in queries], sampling_params)\n",
        "for pred in preds:\n",
        "  print(\"Model prediction: {0}\".format(pred.outputs[0].text))"
      ],
      "metadata": {
        "id": "kFL8a_HWsOGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"I like Avocado.\"\"\""
      ],
      "metadata": {
        "id": "wdEzJxySsQHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def format_prompt_p(input, paragraph=paragraph):\n",
        "  prompt = \"### Instruction:\\n{0}\\n\\n### Response:\\n\".format(input)\n",
        "  if paragraph is not None:\n",
        "    prompt += \"[Retrieval]{0}\".format(paragraph)\n",
        "  return prompt\n",
        "\n",
        "query_1 = \"Leave odd one out: twitter, instagram, whatsapp.\"\n",
        "query_2 = \"Can you tell me the difference between llamas and alpacas?\"\n",
        "queries = [query_1, query_2]\n",
        "\n",
        "# for a query that doesn't require retrieval\n",
        "preds = model.generate([format_prompt_p(query) for query in queries], sampling_params)\n",
        "for pred in preds:\n",
        "  print(\"Model prediction: {0}\".format(pred.outputs[0].text))"
      ],
      "metadata": {
        "id": "TdjTM01PsQpC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}