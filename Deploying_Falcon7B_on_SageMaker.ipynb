{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6BV616dtWApv",
        "GWyP01zgMI3r",
        "UsXcwx9MMDd7",
        "ouZf5rK8L6Ky"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Requirements\n"
      ],
      "metadata": {
        "id": "6BV616dtWApv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sagemaker python-dotenv --quiet"
      ],
      "metadata": {
        "id": "05hMgR3hD2NJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f668a8-d2b9-41a2-aad2-162d2627685f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/844.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m542.7/844.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m844.7/844.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sagemaker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for protobuf3-to-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "S-jarnrCZslB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 - AWS Configuration"
      ],
      "metadata": {
        "id": "GWyP01zgMI3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# environment variables\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG6fZtY3DtGZ",
        "outputId": "237814c7-a99d-4118-c677-542e99bcc41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ### Secret keys"
      ],
      "metadata": {
        "id": "gHKSkn3jcxcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REGION_NAME = \"ap-south-1\"\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = REGION_NAME\n",
        "ROLE_NAME =  'Sagemaker-ExecutionRole'\n",
        "\n",
        "auth_arguments = {\n",
        "    'aws_access_key_id':os.environ[\"aws_access_key_id\"],\n",
        "    'aws_secret_access_key':os.environ[\"aws_secret_access_key\"],\n",
        "    'region_name':REGION_NAME\n",
        "}"
      ],
      "metadata": {
        "id": "7lQjG0DGPwKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[IAM role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)"
      ],
      "metadata": {
        "id": "pDJJRkX7EMPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iam = boto3.client('iam', **auth_arguments)\n",
        "role = iam.get_role(RoleName=ROLE_NAME)['Role']['Arn']\n",
        "\n",
        "session = sagemaker.Session(boto3.Session(**auth_arguments))"
      ],
      "metadata": {
        "id": "geOptch-Ep-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deployment"
      ],
      "metadata": {
        "id": "UsXcwx9MMDd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
        "\n",
        "# image uri\n",
        "llm_image = get_huggingface_llm_image_uri(\"huggingface\")\n",
        "\n",
        "print(f\"image uri: {llm_image}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVUJhmZ0EOja",
        "outputId": "5d138576-2741-4560-da36-b531215c22a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.0-tgi0.8.2-gpu-py39-cu118-ubuntu20.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.huggingface import HuggingFaceModel\n",
        "\n",
        "# Falcon 7b\n",
        "hub = {'HF_MODEL_ID':'tiiuae/falcon-7b'}\n",
        "\n",
        "# Hugging Face Model Class\n",
        "huggingface_model = HuggingFaceModel(\n",
        "   env=hub,\n",
        "   role=role,  # iam role from AWS\n",
        "   image_uri=llm_image,\n",
        "   sagemaker_session=session\n",
        ")"
      ],
      "metadata": {
        "id": "ZaWhGsUy3Cb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy model to SageMaker\n",
        "predictor = huggingface_model.deploy(\n",
        "\tinitial_instance_count=1, # number of instances\n",
        "\tinstance_type='ml.g5.2xlarge', #'ml.g5.4xlarge'\n",
        " \tcontainer_startup_health_check_timeout=300\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEWp5OSEGuEl",
        "outputId": "7e14cc3c-ee76-4068-a16d-08c042999bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inferencing Model"
      ],
      "metadata": {
        "id": "ouZf5rK8L6Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define prompt\n",
        "prompt = \"\"\"You are the most advanced AI assistant on the planet, called Falcon.\n",
        "\n",
        "User: How can we set up Kubernetes cluster on AWS? Think step by step.\n",
        "Falcon:\"\"\"\n",
        "\n",
        "# hyperparameters for llm\n",
        "request = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9,\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# request to endpoint\n",
        "response = predictor.predict(request)\n",
        "\n",
        "# model response\n",
        "assistant = response[0][\"generated_text\"][len(prompt):]"
      ],
      "metadata": {
        "id": "1E8NHDnN0OI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(assistant)"
      ],
      "metadata": {
        "id": "8lNOJoaf3Dtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DELETE ENDPOINT to avoid unnecessary expenses\n",
        "predictor.delete_model()\n",
        "predictor.delete_endpoint()"
      ],
      "metadata": {
        "id": "NieX7MHHdhCQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}